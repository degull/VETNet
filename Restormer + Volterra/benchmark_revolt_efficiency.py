# ============================================================
# âœ… benchmark_revolt_efficiency.py â€” Restormer + Volterra (ReVolT)
#   - Params / FLOPs / Iter/s / Time(ms) / VRAM(MB)
#   - CPU & CUDA ì§€ì›, AMP ì‚¬ìš©(ê°€ëŠ¥ ì‹œ)
#   - models íŒ¨í‚¤ì§€ ìë™ íŒ¨ì¹˜(__init__.py ìƒì„±)
# ============================================================

import os
import sys
import time
import contextlib

import torch
import torch.nn as nn

# ptflopsëŠ” ì„ íƒì‚¬í•­ì´ì§€ë§Œ, ìˆìœ¼ë©´ FLOPs ê³„ì‚° ì‹œë„
try:
    from ptflops import get_model_complexity_info
    _HAS_PTFLOPS = True
except Exception:
    _HAS_PTFLOPS = False


# ------------------------------------------------------------
# âœ… ê²½ë¡œ ì„¤ì • (í˜„ì¬ íŒŒì¼ ê¸°ì¤€)
# ------------------------------------------------------------
CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
PROJECT_DIR = CURRENT_DIR                          # ì´ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ë£¨íŠ¸ì— ë‘˜ ê²ƒì„ ê°€ì •
MODELS_DIR = os.path.join(PROJECT_DIR, "models")

for p in [PROJECT_DIR, MODELS_DIR]:
    if p not in sys.path:
        sys.path.append(p)

# modelsë¥¼ íŒ¨í‚¤ì§€ë¡œ ì¸ì‹ì‹œí‚¤ê¸° (ì—†ìœ¼ë©´ ìƒì„±)
init_path = os.path.join(MODELS_DIR, "__init__.py")
if not os.path.exists(init_path):
    with open(init_path, "w", encoding="utf-8") as f:
        f.write("# autogenerated to mark models as a package\n")
    print("ğŸ§© Patched: created models/__init__.py")


# ------------------------------------------------------------
# âœ… ëª¨ë¸ import
# ------------------------------------------------------------
try:
    from models.restormer_volterra import RestormerVolterra
except Exception as e:
    raise ImportError(f"âŒ ëª¨ë¸ ì„í¬íŠ¸ ì‹¤íŒ¨: {e}")


# ------------------------------------------------------------
# âœ… ì¥ì¹˜/AMP ì„¤ì •
# ------------------------------------------------------------
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
use_amp = (device.type == "cuda")

torch.backends.cudnn.benchmark = True if device.type == "cuda" else False

print("=" * 60)
print(f"Device: {device}")
print(f"AMP enabled: {use_amp}")
print("=" * 60)


# ------------------------------------------------------------
# âœ… ëª¨ë¸ ì´ˆê¸°í™”
#    - ì›ë³¸ êµ¬ì¡° ê·¸ëŒ€ë¡œ (ì´ë¯¸ì§€ ë³µì›/ë³€í™˜ìš©)
# ------------------------------------------------------------
model = RestormerVolterra(
    in_channels=3,
    out_channels=3,
    dim=48,
    num_blocks=[4, 6, 6, 8],
    num_refinement_blocks=4,
    heads=[1, 2, 4, 8],
    ffn_expansion_factor=2.66,
    bias=False,
    LayerNorm_type='WithBias',
    volterra_rank=4
).to(device)
model.eval()

# íŒŒë¼ë¯¸í„° ìˆ˜
num_params = sum(p.numel() for p in model.parameters()) / 1e6
print(f"Total Params: {num_params:.2f}M")


# ------------------------------------------------------------
# âœ… FLOPs ê³„ì‚° (ê°€ëŠ¥í•œ ê²½ìš°ë§Œ)
#   - ì…ë ¥ í¬ê¸°: 256Ã—256 (ì•ˆì „í•œ ì •ì‚¬ê° ì…ë ¥)
# ------------------------------------------------------------
def safe_get_flops(mdl, input_res=(3, 256, 256)):
    if not _HAS_PTFLOPS:
        print("[WARN] ptflops ë¯¸ì„¤ì¹˜: FLOPs ê³„ì‚°ì„ ìƒëµí•©ë‹ˆë‹¤.")
        return 0.0

    # ptflopsê°€ ì¼ë¶€ ì—°ì‚°ì—ì„œ ì‹¤íŒ¨í•  ìˆ˜ ìˆìœ¼ë‹ˆ ë°©ì–´ì ìœ¼ë¡œ
    try:
        # AMPëŠ” ptflops ì¶”ì •ì— í•„ìš” ì—†ë‹¤. ì˜¤íˆë ¤ í˜¼ì„  ë°©ì§€ë¥¼ ìœ„í•´ ë”
        macs, _ = get_model_complexity_info(
            mdl, input_res, as_strings=False, print_per_layer_stat=False, verbose=False
        )
        return (macs or 0.0) / 1e9
    except Exception as e:
        print("Flops estimation was not finished successfully because of the following exception: ")
        print(type(e), ":", e)
        return 0.0

flops_g = safe_get_flops(model, input_res=(3, 256, 256))


# ------------------------------------------------------------
# âœ… ì†ë„ / VRAM ì¸¡ì •
#   - ì…ë ¥: 1Ã—3Ã—256Ã—256 (ë³µì›ê³„ì—´ ëª¨ë¸ìš©)
#   - AMP & CUDA synchronize
# ------------------------------------------------------------
B, C, H, W = 1, 3, 256, 256
dummy_input = torch.randn(B, C, H, W, device=device)

# ë©”ëª¨ë¦¬ í†µê³„ ì´ˆê¸°í™” (CUDA ì „ìš©)
if device.type == "cuda":
    torch.cuda.reset_peak_memory_stats()
    torch.cuda.synchronize()

# ì›Œë°ì—…
with torch.no_grad():
    with torch.autocast(device_type="cuda", dtype=torch.float16) if use_amp else contextlib.nullcontext():
        for _ in range(10):
            _ = model(dummy_input)
    if device.type == "cuda":
        torch.cuda.synchronize()

# íƒ€ì´ë°
n_iters = 50
start = time.time()
with torch.no_grad():
    with torch.autocast(device_type="cuda", dtype=torch.float16) if use_amp else contextlib.nullcontext():
        for _ in range(n_iters):
            _ = model(dummy_input)
if device.type == "cuda":
    torch.cuda.synchronize()
elapsed = (time.time() - start) / n_iters

iter_per_sec = 1.0 / elapsed
time_ms = elapsed * 1000.0
vram_mb = (torch.cuda.max_memory_allocated(device) / 1024**2) if device.type == "cuda" else 0.0


# ------------------------------------------------------------
# âœ… ê²°ê³¼ ì¶œë ¥
# ------------------------------------------------------------
print("\n================ Efficiency Benchmark Results ================")
print(f"{'Model':<24}{'Params(M)':>10}{'FLOPs(G)':>10}{'Iter/s':>10}{'Time(ms)':>12}{'VRAM(MB)':>12}")
print("-" * 78)
print(f"{'ReVolT (Restormer+Volterra)':<24}{num_params:>10.2f}{flops_g:>10.2f}{iter_per_sec:>10.2f}{time_ms:>12.2f}{vram_mb:>12.2f}")
print("=" * 78)
print("[Guide] ë†’ì„ìˆ˜ë¡ ì¢‹ì€ ì§€í‘œ: Iter/s")
print("[Guide] ë‚®ì„ìˆ˜ë¡ ì¢‹ì€ ì§€í‘œ: Params, FLOPs, Time(ms), VRAM(MB)")
print("=" * 78)
